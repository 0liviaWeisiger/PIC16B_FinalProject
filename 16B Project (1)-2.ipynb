{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174e390b",
   "metadata": {},
   "source": [
    "# Predicting United States Real Estate Prices\n",
    "By: Grace Li, Olivia Weisiger and Fionnuala Eastwood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054bf35",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. Dataset Preprocessing and Merging\n",
    "2. Data Visualization\n",
    "3. Classic Machine Learning Models\n",
    "4. Deep Learning Models\n",
    "5. Analysis of accuracy and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6a3cb",
   "metadata": {},
   "source": [
    "### Context\n",
    "The United States housing market is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ff604",
   "metadata": {},
   "source": [
    "### Our Goal\n",
    "\n",
    "This project will explore the current United States real-estate market, investigate what factors influence the price of property, and create multiple machine learning models that predict these housing costs throughout the country. More specifically, this will be accomplished through implementation of (add briefly about what models we end up using....) Being able to infer and understand the trends of real estate is extremely valuable economic knowledge that will provide important insights about our country. \n",
    "\n",
    "Furthermore, our project aims to deepen our understanding of how societal biases influence external structures such as the economy. By merging datasets, we will investigate which underlying factors such as (add briefly when choose other data) affect the prices of houses in order to draw deeper conclusions about intangible factors impacting our economic climate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5441e62",
   "metadata": {},
   "source": [
    "### Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d47e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 23:11:29.581579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b767510",
   "metadata": {},
   "source": [
    "This data is from Kaggle's \"USA Real Estate Dataset\" found here: https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c01278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('realtor-data.zip.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba8cab",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "Let's first break down what our dataset looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3990b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2226382, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881d372",
   "metadata": {},
   "source": [
    "We have a dataset with over 2 million rows and 12 columns. Since this is way too many samples to process in a reasonable computational time, we will take a random subset of 100,000 of these samples to perform analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac630652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 12)\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "df = df.sample(50000)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95024cfe",
   "metadata": {},
   "source": [
    "With our refined sample, let's get an idea of what our dataset looks like by outputting a few rows of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "111beab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brokered_by</th>\n",
       "      <th>status</th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>prev_sold_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1201130</th>\n",
       "      <td>62531.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>585407.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>735851.0</td>\n",
       "      <td>Queen Creek</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>85142.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387359</th>\n",
       "      <td>21986.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>59500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.57</td>\n",
       "      <td>1285890.0</td>\n",
       "      <td>Roberta</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>31078.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174460</th>\n",
       "      <td>59831.0</td>\n",
       "      <td>sold</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1105743.0</td>\n",
       "      <td>Davis</td>\n",
       "      <td>California</td>\n",
       "      <td>95618.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2022-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776215</th>\n",
       "      <td>10595.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>298000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>961740.0</td>\n",
       "      <td>Sumner</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233886</th>\n",
       "      <td>103011.0</td>\n",
       "      <td>for_sale</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>564581.0</td>\n",
       "      <td>Edgewood</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>87015.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         brokered_by    status     price  bed  bath  acre_lot     street  \\\n",
       "1201130      62531.0  for_sale  585407.0  4.0   3.0      0.15   735851.0   \n",
       "387359       21986.0  for_sale   59500.0  NaN   NaN     18.57  1285890.0   \n",
       "2174460      59831.0      sold  230000.0  3.0   2.0       NaN  1105743.0   \n",
       "776215       10595.0  for_sale  298000.0  3.0   2.0      1.55   961740.0   \n",
       "1233886     103011.0  for_sale  150000.0  NaN   NaN      0.75   564581.0   \n",
       "\n",
       "                city       state  zip_code  house_size prev_sold_date  \n",
       "1201130  Queen Creek     Arizona   85142.0      2186.0            NaN  \n",
       "387359       Roberta     Georgia   31078.0         NaN     2022-04-29  \n",
       "2174460        Davis  California   95618.0      1344.0     2022-01-04  \n",
       "776215        Sumner        Iowa   50674.0      1240.0            NaN  \n",
       "1233886     Edgewood  New Mexico   87015.0         NaN            NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c8b99",
   "metadata": {},
   "source": [
    "Notice that each sample in the dataset is a real estate listing in the United States (the listings are all from 2022-2024), and each sample has 12 features that provide numerical or categorical information about the listing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dd10b",
   "metadata": {},
   "source": [
    "Here is an overview of each feature's meaning and data type:\n",
    "\n",
    "- brokered_by:\n",
    "\n",
    "- status:\n",
    "\n",
    "- price:\n",
    "\n",
    "- bed:\n",
    "\n",
    "- bath:\n",
    "\n",
    "- etc fill in later (look on kaggle these descriptins are provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9f616",
   "metadata": {},
   "source": [
    "Now that we have an understanding of our data set, we will perform some processing on the data so that it is cleaner to use. Firstly, we will drop some unnecessary columns that do not contribute to our analysis goals. The brokered_by column which encodes the real-estate company in charge of the property is not necessary because we are interested in the qualities of the house itself. Additionally, the status column is not needed because we will use the price set for the house equivalently regardless if it is sold or for sale. Lastly, the previously sold date can be dropped since we are focused on the current selling price. We will trim our dataset from 12 columns to 9 with these modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f20f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['brokered_by', 'status', 'prev_sold_date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86679a9",
   "metadata": {},
   "source": [
    "This dataset contains listings from the United States and all it's territories. For our purposes, we only want to analyze data from the 50 states (and Washington, DC) so let's trim out samples taken from Puerto Rico and the Virgin Islands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ac992a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['state'] != \"Puerto Rico\") & (df['state'] != \"Virgin Islands\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb20ef2",
   "metadata": {},
   "source": [
    "Our next processing step is making sure we don't have any NaN's in our dataset, as empty data values might impact our analysis models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdd41e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42323\n"
     ]
    }
   ],
   "source": [
    "#sum up all NaN values present in dataset (in any feature column)\n",
    "print (df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe21dbe",
   "metadata": {},
   "source": [
    "We see that we have some data entries with no value, so let's remove all rows that contain any NaN values. We will also check the shape of our data frame after this removal to make sure we still have plenty of samples to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed4f3eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have: 0 NaN entires\n",
      "Our new dataset shape is (30425, 9)\n"
     ]
    }
   ],
   "source": [
    "#remove all rows missing data\n",
    "df = df.dropna()\n",
    "\n",
    "#verify we now have no NaN values, expect a value of zero\n",
    "print (f'We now have: {df.isnull().sum().sum()} NaN entires')\n",
    "\n",
    "#print new shape\n",
    "print (f'Our new dataset shape is {df.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3699592",
   "metadata": {},
   "source": [
    "We successfully dropped all empty entries and still have a substantial size data frame to analyze.\n",
    "\n",
    "Our last step in data processing is preparing our target price data for our future machine learning models. We noticed that predicting the price to an exact number (as the current column does) is quite specific, so instead in some cases we will want to predict whether any piece of real-estate is more generally expensive or cheap. The next question that follows is how we will quantify this \"expensive\" vs \"cheap\". \n",
    "\n",
    "Our natural thought was just categorizing the samples based on if they were on the higher half of all in our dataset vs the lowest. However, upon further analysis we realize that the state the property is in has an overwhelmingly powerful influence on this categorization. For example we would see that practically all samples from New York would fall in the upper portion of data, while a huge majority of samples from rural states will be in the lower. This would leave our model with very little to do, so to work around this we have decided to create a categorical column that contains a 1 if the property price is above the median housing price **of the state it is in**, and a 0 if the property is below this median average of its state. This takes out the state bias and may lead to more informative conclusions about other features that are no longer overshadowed.\n",
    "\n",
    "To do this, we first merge data with the median housing prices by state. This data was taken from the following link: https://www.bankrate.com/real-estate/median-home-price/#how-much, and is up to date as of November 2023 (which is the same time period our real-estate data was taken from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c87444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>med_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585407.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>735851.0</td>\n",
       "      <td>Queen Creek</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>85142.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>$435,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>961740.0</td>\n",
       "      <td>Sumner</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>$289,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>930049.0</td>\n",
       "      <td>Spring Valley</td>\n",
       "      <td>California</td>\n",
       "      <td>91977.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>$793,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1826673.0</td>\n",
       "      <td>Maggie Valley</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28751.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>$362,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>872293.0</td>\n",
       "      <td>Bigfork</td>\n",
       "      <td>Montana</td>\n",
       "      <td>59911.0</td>\n",
       "      <td>2822.0</td>\n",
       "      <td>$609,900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bed  bath  acre_lot     street           city           state  \\\n",
       "0  585407.0  4.0   3.0      0.15   735851.0    Queen Creek         Arizona   \n",
       "1  298000.0  3.0   2.0      1.55   961740.0         Sumner            Iowa   \n",
       "2  739900.0  4.0   2.0      0.25   930049.0  Spring Valley      California   \n",
       "3  315000.0  3.0   2.0      0.42  1826673.0  Maggie Valley  North Carolina   \n",
       "4  855000.0  3.0   2.0      0.22   872293.0        Bigfork         Montana   \n",
       "\n",
       "   zip_code  house_size med_price  \n",
       "0   85142.0      2186.0  $435,300  \n",
       "1   50674.0      1240.0  $289,900  \n",
       "2   91977.0      1340.0  $793,600  \n",
       "3   28751.0      1344.0  $362,200  \n",
       "4   59911.0      2822.0  $609,900  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload data\n",
    "df_med = pd.read_csv('median_prices.csv')\n",
    "df_med.head()\n",
    "\n",
    "#select state and median price columns we want\n",
    "df_med = df_med[[\"state\", \"med_price\"]]\n",
    "\n",
    "#merge along state column\n",
    "df = pd.merge(df, df_med, on = [\"state\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43110e9c",
   "metadata": {},
   "source": [
    "Notice one more problem exists: we have a price listed in string form with dollar sign and commas. Instead we want it to be numerical in order to compare it with our current price column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f5152e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dollar signs and commas, then convert to integers\n",
    "df['med_price'] = df['med_price'].replace({'\\$': '', ',': ''}, regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776bdde",
   "metadata": {},
   "source": [
    "Lastly, we want to create a new column which we will call above_average. This column will contain a 1 if the price of that sample is above the median price in the state, and a 0 if it is below. We will also remove the median price column afterwards because it served its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be76f33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>above_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585407.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>735851.0</td>\n",
       "      <td>Queen Creek</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>85142.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>961740.0</td>\n",
       "      <td>Sumner</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>930049.0</td>\n",
       "      <td>Spring Valley</td>\n",
       "      <td>California</td>\n",
       "      <td>91977.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1826673.0</td>\n",
       "      <td>Maggie Valley</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28751.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>872293.0</td>\n",
       "      <td>Bigfork</td>\n",
       "      <td>Montana</td>\n",
       "      <td>59911.0</td>\n",
       "      <td>2822.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bed  bath  acre_lot     street           city           state  \\\n",
       "0  585407.0  4.0   3.0      0.15   735851.0    Queen Creek         Arizona   \n",
       "1  298000.0  3.0   2.0      1.55   961740.0         Sumner            Iowa   \n",
       "2  739900.0  4.0   2.0      0.25   930049.0  Spring Valley      California   \n",
       "3  315000.0  3.0   2.0      0.42  1826673.0  Maggie Valley  North Carolina   \n",
       "4  855000.0  3.0   2.0      0.22   872293.0        Bigfork         Montana   \n",
       "\n",
       "   zip_code  house_size  above_average  \n",
       "0   85142.0      2186.0              1  \n",
       "1   50674.0      1240.0              1  \n",
       "2   91977.0      1340.0              0  \n",
       "3   28751.0      1344.0              0  \n",
       "4   59911.0      2822.0              1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['above_average'] = df.apply(lambda row: 1 if row['price'] > row['med_price'] else 0, axis=1)\n",
    "df = df.drop('med_price', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429dfae",
   "metadata": {},
   "source": [
    "This looks good, now we are ready to merge with other data sets to add more features to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6f273",
   "metadata": {},
   "source": [
    "### Dataset Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa728de8",
   "metadata": {},
   "source": [
    "While the relationship between features such as number of rooms or number of acres on real-estate prices is quite intuitive, this project aims to delve beyond these variables and investigate more abstract influences. This will be done by merging our current dataframe with new datasets in order to add features including minimum wage of the state, median income by zip code, and even political affiliation, as we are curious if any of these variables will display a strong correlation with housing prices. One caution to note is that our original real estate data is from the past two years, so we will need to make sure the data we are merging with is taken from the same time period in order to obtain accurate conclusions.\n",
    "\n",
    "The first dataset we will merge with is Kaggle's \"US Household Income by Zip Code 2021-2011\" found here: https://www.kaggle.com/datasets/claygendron/us-household-income-by-zip-code-2021-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad3e13c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIP</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Geographic Area Name</th>\n",
       "      <th>Households</th>\n",
       "      <th>Households Margin of Error</th>\n",
       "      <th>Households Less Than $10,000</th>\n",
       "      <th>Households Less Than $10,000 Margin of Error</th>\n",
       "      <th>Households $10,000 to $14,999</th>\n",
       "      <th>Households $10,000 to $14,999 Margin of Error</th>\n",
       "      <th>Households $15,000 to $24,999</th>\n",
       "      <th>...</th>\n",
       "      <th>Nonfamily Households $150,000 to $199,999</th>\n",
       "      <th>Nonfamily Households $150,000 to $199,999 Margin of Error</th>\n",
       "      <th>Nonfamily Households $200,000 or More</th>\n",
       "      <th>Nonfamily Households $200,000 or More Margin of Error</th>\n",
       "      <th>Nonfamily Households Median Income (Dollars)</th>\n",
       "      <th>Nonfamily Households Median Income (Dollars) Margin of Error</th>\n",
       "      <th>Nonfamily Households Mean Income (Dollars)</th>\n",
       "      <th>Nonfamily Households Mean Income (Dollars) Margin of Error</th>\n",
       "      <th>Nonfamily Households Nonfamily Income in the Past 12 Months</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>860Z200US00601</td>\n",
       "      <td>ZCTA5 00601</td>\n",
       "      <td>5397.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>9386.0</td>\n",
       "      <td>1472.0</td>\n",
       "      <td>13044.0</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>860Z200US00602</td>\n",
       "      <td>ZCTA5 00602</td>\n",
       "      <td>12858.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>12.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>11242.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>16419.0</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>860Z200US00603</td>\n",
       "      <td>ZCTA5 00603</td>\n",
       "      <td>19295.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>32.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>13.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>17.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10639.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>16824.0</td>\n",
       "      <td>2217.0</td>\n",
       "      <td>34.9</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>860Z200US00606</td>\n",
       "      <td>ZCTA5 00606</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>13.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>23.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>15849.0</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>16312.0</td>\n",
       "      <td>2662.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>860Z200US00610</td>\n",
       "      <td>ZCTA5 00610</td>\n",
       "      <td>8934.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>13.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>12832.0</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>16756.0</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ZIP       Geography Geographic Area Name  Households  \\\n",
       "0  601  860Z200US00601          ZCTA5 00601      5397.0   \n",
       "1  602  860Z200US00602          ZCTA5 00602     12858.0   \n",
       "2  603  860Z200US00603          ZCTA5 00603     19295.0   \n",
       "3  606  860Z200US00606          ZCTA5 00606      1968.0   \n",
       "4  610  860Z200US00610          ZCTA5 00610      8934.0   \n",
       "\n",
       "   Households Margin of Error  Households Less Than $10,000  \\\n",
       "0                       264.0                          33.2   \n",
       "1                       448.0                          27.1   \n",
       "2                       555.0                          32.1   \n",
       "3                       171.0                          28.4   \n",
       "4                       372.0                          20.5   \n",
       "\n",
       "   Households Less Than $10,000 Margin of Error  \\\n",
       "0                                           4.4   \n",
       "1                                           2.9   \n",
       "2                                           2.5   \n",
       "3                                           5.5   \n",
       "4                                           2.5   \n",
       "\n",
       "   Households $10,000 to $14,999  \\\n",
       "0                           15.7   \n",
       "1                           12.7   \n",
       "2                           13.4   \n",
       "3                           13.3   \n",
       "4                           13.2   \n",
       "\n",
       "   Households $10,000 to $14,999 Margin of Error  \\\n",
       "0                                            2.9   \n",
       "1                                            2.1   \n",
       "2                                            1.6   \n",
       "3                                            4.4   \n",
       "4                                            2.5   \n",
       "\n",
       "   Households $15,000 to $24,999  ...  \\\n",
       "0                           23.9  ...   \n",
       "1                           20.5  ...   \n",
       "2                           17.2  ...   \n",
       "3                           23.3  ...   \n",
       "4                           23.3  ...   \n",
       "\n",
       "   Nonfamily Households $150,000 to $199,999  \\\n",
       "0                                        0.0   \n",
       "1                                        0.0   \n",
       "2                                        0.6   \n",
       "3                                        0.0   \n",
       "4                                        0.0   \n",
       "\n",
       "   Nonfamily Households $150,000 to $199,999 Margin of Error  \\\n",
       "0                                                2.8           \n",
       "1                                                1.3           \n",
       "2                                                0.6           \n",
       "3                                                7.5           \n",
       "4                                                1.8           \n",
       "\n",
       "   Nonfamily Households $200,000 or More  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.2   \n",
       "3                                    0.0   \n",
       "4                                    0.0   \n",
       "\n",
       "   Nonfamily Households $200,000 or More Margin of Error  \\\n",
       "0                                                2.8       \n",
       "1                                                1.3       \n",
       "2                                                0.4       \n",
       "3                                                7.5       \n",
       "4                                                1.8       \n",
       "\n",
       "   Nonfamily Households Median Income (Dollars)  \\\n",
       "0                                        9386.0   \n",
       "1                                       11242.0   \n",
       "2                                       10639.0   \n",
       "3                                       15849.0   \n",
       "4                                       12832.0   \n",
       "\n",
       "   Nonfamily Households Median Income (Dollars) Margin of Error  \\\n",
       "0                                             1472.0              \n",
       "1                                             1993.0              \n",
       "2                                              844.0              \n",
       "3                                             3067.0              \n",
       "4                                             2405.0              \n",
       "\n",
       "   Nonfamily Households Mean Income (Dollars)  \\\n",
       "0                                     13044.0   \n",
       "1                                     16419.0   \n",
       "2                                     16824.0   \n",
       "3                                     16312.0   \n",
       "4                                     16756.0   \n",
       "\n",
       "   Nonfamily Households Mean Income (Dollars) Margin of Error  \\\n",
       "0                                             1949.0            \n",
       "1                                             2310.0            \n",
       "2                                             2217.0            \n",
       "3                                             2662.0            \n",
       "4                                             1740.0            \n",
       "\n",
       "   Nonfamily Households Nonfamily Income in the Past 12 Months    Year  \n",
       "0                                               15.0            2021.0  \n",
       "1                                               20.1            2021.0  \n",
       "2                                               34.9            2021.0  \n",
       "3                                               13.0            2021.0  \n",
       "4                                               14.5            2021.0  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_income = pd.read_csv('us_income_zipcode.csv')\n",
    "df_income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c97b50",
   "metadata": {},
   "source": [
    "This dataset contains the results of the 2011 and 2021 national census, and we have chosen it in order to add a median income feature to our real estate pricing dataset. As explained above, we are only interested in the 2021 data since our pricing data comes from recent years, so we will trim down our dataset accordingly. Additionally, the dataset comes with dozens of feature columns, but for our purposes we only need to keep the zip code column (which we will use to merge our original dataset), and the median household income column. So let's process our dataset and display the cleaner result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0114bfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIP</th>\n",
       "      <th>Nonfamily Households Median Income (Dollars)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>9386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>11242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>10639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>15849.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>12832.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ZIP  Nonfamily Households Median Income (Dollars)\n",
       "0  601                                        9386.0\n",
       "1  602                                       11242.0\n",
       "2  603                                       10639.0\n",
       "3  606                                       15849.0\n",
       "4  610                                       12832.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select only samples from most recent census\n",
    "df_income = df_income[df_income[\"Year\"] == 2021.0]\n",
    "\n",
    "#select only features we want\n",
    "df_income = df_income[[\"ZIP\", \"Nonfamily Households Median Income (Dollars)\"]]\n",
    "\n",
    "df_income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56303003",
   "metadata": {},
   "source": [
    "Now we are ready to merge with our original dataset. Currently our zip code columns have different names so we will rename them identically, and they also have different types (integer vs float) so we will convert to a float variable to avoid type error interference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7208a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income[\"ZIP\"] = df_income[\"ZIP\"].astype(float)\n",
    "\n",
    "df_income = df_income.rename(columns={'ZIP': 'zip_code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394f873",
   "metadata": {},
   "source": [
    "We will use an inner merge (explain why...)\n",
    "The census data was very thorough (we have very few NaN values), so we can just remove any empty data rows and our dataset remains practically the same. We verify this assumption by outputting our dataset shape after the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b985056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30081, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>above_average</th>\n",
       "      <th>Nonfamily Households Median Income (Dollars)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585407.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>735851.0</td>\n",
       "      <td>Queen Creek</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>85142.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>961740.0</td>\n",
       "      <td>Sumner</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>930049.0</td>\n",
       "      <td>Spring Valley</td>\n",
       "      <td>California</td>\n",
       "      <td>91977.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1826673.0</td>\n",
       "      <td>Maggie Valley</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28751.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>872293.0</td>\n",
       "      <td>Bigfork</td>\n",
       "      <td>Montana</td>\n",
       "      <td>59911.0</td>\n",
       "      <td>2822.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42860.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bed  bath  acre_lot     street           city           state  \\\n",
       "0  585407.0  4.0   3.0      0.15   735851.0    Queen Creek         Arizona   \n",
       "1  298000.0  3.0   2.0      1.55   961740.0         Sumner            Iowa   \n",
       "2  739900.0  4.0   2.0      0.25   930049.0  Spring Valley      California   \n",
       "3  315000.0  3.0   2.0      0.42  1826673.0  Maggie Valley  North Carolina   \n",
       "4  855000.0  3.0   2.0      0.22   872293.0        Bigfork         Montana   \n",
       "\n",
       "   zip_code  house_size  above_average  \\\n",
       "0   85142.0      2186.0              1   \n",
       "1   50674.0      1240.0              1   \n",
       "2   91977.0      1340.0              0   \n",
       "3   28751.0      1344.0              0   \n",
       "4   59911.0      2822.0              1   \n",
       "\n",
       "   Nonfamily Households Median Income (Dollars)  \n",
       "0                                       54890.0  \n",
       "1                                       40625.0  \n",
       "2                                       50286.0  \n",
       "3                                       42108.0  \n",
       "4                                       42860.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_income, on = [\"zip_code\"])\n",
    "\n",
    "#remove all rows missing data\n",
    "df = df.dropna()\n",
    "\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446b0e0",
   "metadata": {},
   "source": [
    "This feature looks good, let's move on to some more merges.\n",
    "\n",
    "Next, we want to add to our dataset statistics on political affiliation by state and minimum wage by state, which should be slightly simpler than merging by zipcode. \n",
    "\n",
    "First we will use is Kaggle's \"2020 US Presidential Election Results by State\" linked here: https://www.kaggle.com/datasets/callummacpherson14/2020-us-presidential-election-results-by-state. This data was taken appropriately recently to match our real-estate data, and it contains voting percentage and win vs loss data on Biden and Trump from the 2020 election.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70b72055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>state_abr</th>\n",
       "      <th>trump_pct</th>\n",
       "      <th>biden_pct</th>\n",
       "      <th>trump_vote</th>\n",
       "      <th>biden_vote</th>\n",
       "      <th>trump_win</th>\n",
       "      <th>biden_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "      <td>53.1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>189543</td>\n",
       "      <td>153502</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>HI</td>\n",
       "      <td>34.3</td>\n",
       "      <td>63.7</td>\n",
       "      <td>196864</td>\n",
       "      <td>366130</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington</td>\n",
       "      <td>WA</td>\n",
       "      <td>39.0</td>\n",
       "      <td>58.4</td>\n",
       "      <td>1584651</td>\n",
       "      <td>2369612</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>OR</td>\n",
       "      <td>40.7</td>\n",
       "      <td>56.9</td>\n",
       "      <td>958448</td>\n",
       "      <td>1340383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "      <td>34.3</td>\n",
       "      <td>63.5</td>\n",
       "      <td>5982194</td>\n",
       "      <td>11082293</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state state_abr  trump_pct  biden_pct  trump_vote  biden_vote  \\\n",
       "0      Alaska        AK       53.1       43.0      189543      153502   \n",
       "1      Hawaii        HI       34.3       63.7      196864      366130   \n",
       "2  Washington        WA       39.0       58.4     1584651     2369612   \n",
       "3      Oregon        OR       40.7       56.9      958448     1340383   \n",
       "4  California        CA       34.3       63.5     5982194    11082293   \n",
       "\n",
       "   trump_win  biden_win  \n",
       "0          1          0  \n",
       "1          0          1  \n",
       "2          0          1  \n",
       "3          0          1  \n",
       "4          0          1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_election = pd.read_csv('voting.csv.xls')\n",
    "df_election.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0254d",
   "metadata": {},
   "source": [
    "Notice this is quite a clean dataset already, all we need to do is select the columns we are interested in and perform another inner merge along the column column of state. Here, we will choose to keep the state column which is needed for the merge, as well as the Trump pct, Biden pct columns since these provide more detailed information then the binary win vs loss columns. Let's do so and check our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "755c8900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We still have: 0 NaN entires\n",
      "(30081, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>above_average</th>\n",
       "      <th>Nonfamily Households Median Income (Dollars)</th>\n",
       "      <th>biden_pct</th>\n",
       "      <th>trump_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585407.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>735851.0</td>\n",
       "      <td>Queen Creek</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>85142.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54890.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>49.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>961740.0</td>\n",
       "      <td>Sumner</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40625.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>53.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>930049.0</td>\n",
       "      <td>Spring Valley</td>\n",
       "      <td>California</td>\n",
       "      <td>91977.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50286.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>34.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1826673.0</td>\n",
       "      <td>Maggie Valley</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28751.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42108.0</td>\n",
       "      <td>48.7</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>872293.0</td>\n",
       "      <td>Bigfork</td>\n",
       "      <td>Montana</td>\n",
       "      <td>59911.0</td>\n",
       "      <td>2822.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42860.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>56.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bed  bath  acre_lot     street           city           state  \\\n",
       "0  585407.0  4.0   3.0      0.15   735851.0    Queen Creek         Arizona   \n",
       "1  298000.0  3.0   2.0      1.55   961740.0         Sumner            Iowa   \n",
       "2  739900.0  4.0   2.0      0.25   930049.0  Spring Valley      California   \n",
       "3  315000.0  3.0   2.0      0.42  1826673.0  Maggie Valley  North Carolina   \n",
       "4  855000.0  3.0   2.0      0.22   872293.0        Bigfork         Montana   \n",
       "\n",
       "   zip_code  house_size  above_average  \\\n",
       "0   85142.0      2186.0              1   \n",
       "1   50674.0      1240.0              1   \n",
       "2   91977.0      1340.0              0   \n",
       "3   28751.0      1344.0              0   \n",
       "4   59911.0      2822.0              1   \n",
       "\n",
       "   Nonfamily Households Median Income (Dollars)  biden_pct  trump_pct  \n",
       "0                                       54890.0       49.4       49.1  \n",
       "1                                       40625.0       45.0       53.2  \n",
       "2                                       50286.0       63.5       34.3  \n",
       "3                                       42108.0       48.7       50.1  \n",
       "4                                       42860.0       40.6       56.9  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select only features we want\n",
    "df_election = df_election[[\"state\", \"biden_pct\", \"trump_pct\"]]\n",
    "\n",
    "#merge dataframe along the column of state\n",
    "df = pd.merge(df, df_election, on = [\"state\"])\n",
    "\n",
    "#verify there were no null data values added\n",
    "print (f'We still have: {df.isnull().sum().sum()} NaN entires')\n",
    "\n",
    "#output model summary\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741de9f7",
   "metadata": {},
   "source": [
    "Notice we still have no NaN entries, so our merging didn't add any problematic data and appears to be successfully added. \n",
    "\n",
    "Finally, we will perform this process one more time in order to add data on what each state's minimum wage is. This time we will use Kaggle's \"Living Wage - State Capitals\" found at https://www.kaggle.com/datasets/brandonconrady/living-wage-state-capitals. We again verified this was taken from the past two years for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "823db446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_territory</th>\n",
       "      <th>city</th>\n",
       "      <th>minimum_wage</th>\n",
       "      <th>one_adult_no_kids_living_wage</th>\n",
       "      <th>one_adult_one_kid_living_wage</th>\n",
       "      <th>one_adult_two_kids_living_wage</th>\n",
       "      <th>one_adult_three_kids_living_wage</th>\n",
       "      <th>two_adults_one_working_no_kids_living_wage</th>\n",
       "      <th>two_adults_one_working_one_kid_living_wage</th>\n",
       "      <th>two_adults_one_working_two_kids_living_wage</th>\n",
       "      <th>...</th>\n",
       "      <th>one_adult_two_kids_poverty_wage</th>\n",
       "      <th>one_adult_three_kids_poverty_wage</th>\n",
       "      <th>two_adults_one_working_no_kids_poverty_wage</th>\n",
       "      <th>two_adults_one_working_one_kid_poverty_wage</th>\n",
       "      <th>two_adults_one_working_two_kids_poverty_wage</th>\n",
       "      <th>two_adults_one_working_three_kids_poverty_wage</th>\n",
       "      <th>two_adults_both_working_no_kids_poverty_wage</th>\n",
       "      <th>two_adults_both_working_one_kid_poverty_wage</th>\n",
       "      <th>two_adults_both_working_two_kids_poverty_wage</th>\n",
       "      <th>two_adults_both_working_three_kids_poverty_wage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>Washington</td>\n",
       "      <td>13.25</td>\n",
       "      <td>19.97</td>\n",
       "      <td>38.95</td>\n",
       "      <td>48.99</td>\n",
       "      <td>63.96</td>\n",
       "      <td>29.61</td>\n",
       "      <td>34.55</td>\n",
       "      <td>38.32</td>\n",
       "      <td>...</td>\n",
       "      <td>10.44</td>\n",
       "      <td>12.60</td>\n",
       "      <td>8.29</td>\n",
       "      <td>10.44</td>\n",
       "      <td>12.60</td>\n",
       "      <td>14.75</td>\n",
       "      <td>4.14</td>\n",
       "      <td>5.22</td>\n",
       "      <td>6.30</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Montgomery</td>\n",
       "      <td>7.25</td>\n",
       "      <td>13.56</td>\n",
       "      <td>27.35</td>\n",
       "      <td>33.42</td>\n",
       "      <td>42.17</td>\n",
       "      <td>22.59</td>\n",
       "      <td>26.66</td>\n",
       "      <td>30.27</td>\n",
       "      <td>...</td>\n",
       "      <td>10.44</td>\n",
       "      <td>12.60</td>\n",
       "      <td>8.29</td>\n",
       "      <td>10.44</td>\n",
       "      <td>12.60</td>\n",
       "      <td>14.75</td>\n",
       "      <td>4.14</td>\n",
       "      <td>5.22</td>\n",
       "      <td>6.30</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Juneau</td>\n",
       "      <td>10.19</td>\n",
       "      <td>15.48</td>\n",
       "      <td>29.99</td>\n",
       "      <td>36.00</td>\n",
       "      <td>47.42</td>\n",
       "      <td>24.48</td>\n",
       "      <td>29.46</td>\n",
       "      <td>33.01</td>\n",
       "      <td>...</td>\n",
       "      <td>13.05</td>\n",
       "      <td>15.75</td>\n",
       "      <td>10.36</td>\n",
       "      <td>13.05</td>\n",
       "      <td>15.75</td>\n",
       "      <td>18.44</td>\n",
       "      <td>5.18</td>\n",
       "      <td>6.53</td>\n",
       "      <td>7.87</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15.41</td>\n",
       "      <td>29.44</td>\n",
       "      <td>35.40</td>\n",
       "      <td>46.01</td>\n",
       "      <td>24.85</td>\n",
       "      <td>29.25</td>\n",
       "      <td>32.98</td>\n",
       "      <td>...</td>\n",
       "      <td>10.44</td>\n",
       "      <td>12.60</td>\n",
       "      <td>8.29</td>\n",
       "      <td>10.44</td>\n",
       "      <td>12.60</td>\n",
       "      <td>14.75</td>\n",
       "      <td>4.14</td>\n",
       "      <td>5.22</td>\n",
       "      <td>6.30</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Little Rock</td>\n",
       "      <td>10.00</td>\n",
       "      <td>13.97</td>\n",
       "      <td>28.81</td>\n",
       "      <td>35.49</td>\n",
       "      <td>45.33</td>\n",
       "      <td>23.21</td>\n",
       "      <td>27.66</td>\n",
       "      <td>31.36</td>\n",
       "      <td>...</td>\n",
       "      <td>10.44</td>\n",
       "      <td>12.60</td>\n",
       "      <td>8.29</td>\n",
       "      <td>10.44</td>\n",
       "      <td>12.60</td>\n",
       "      <td>14.75</td>\n",
       "      <td>4.14</td>\n",
       "      <td>5.22</td>\n",
       "      <td>6.30</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        state_territory         city  minimum_wage  \\\n",
       "0  District of Columbia   Washington         13.25   \n",
       "1               Alabama   Montgomery          7.25   \n",
       "2                Alaska       Juneau         10.19   \n",
       "3               Arizona      Phoenix         12.00   \n",
       "4              Arkansas  Little Rock         10.00   \n",
       "\n",
       "   one_adult_no_kids_living_wage  one_adult_one_kid_living_wage  \\\n",
       "0                          19.97                          38.95   \n",
       "1                          13.56                          27.35   \n",
       "2                          15.48                          29.99   \n",
       "3                          15.41                          29.44   \n",
       "4                          13.97                          28.81   \n",
       "\n",
       "   one_adult_two_kids_living_wage  one_adult_three_kids_living_wage  \\\n",
       "0                           48.99                             63.96   \n",
       "1                           33.42                             42.17   \n",
       "2                           36.00                             47.42   \n",
       "3                           35.40                             46.01   \n",
       "4                           35.49                             45.33   \n",
       "\n",
       "   two_adults_one_working_no_kids_living_wage  \\\n",
       "0                                       29.61   \n",
       "1                                       22.59   \n",
       "2                                       24.48   \n",
       "3                                       24.85   \n",
       "4                                       23.21   \n",
       "\n",
       "   two_adults_one_working_one_kid_living_wage  \\\n",
       "0                                       34.55   \n",
       "1                                       26.66   \n",
       "2                                       29.46   \n",
       "3                                       29.25   \n",
       "4                                       27.66   \n",
       "\n",
       "   two_adults_one_working_two_kids_living_wage  ...  \\\n",
       "0                                        38.32  ...   \n",
       "1                                        30.27  ...   \n",
       "2                                        33.01  ...   \n",
       "3                                        32.98  ...   \n",
       "4                                        31.36  ...   \n",
       "\n",
       "   one_adult_two_kids_poverty_wage  one_adult_three_kids_poverty_wage  \\\n",
       "0                            10.44                              12.60   \n",
       "1                            10.44                              12.60   \n",
       "2                            13.05                              15.75   \n",
       "3                            10.44                              12.60   \n",
       "4                            10.44                              12.60   \n",
       "\n",
       "   two_adults_one_working_no_kids_poverty_wage  \\\n",
       "0                                         8.29   \n",
       "1                                         8.29   \n",
       "2                                        10.36   \n",
       "3                                         8.29   \n",
       "4                                         8.29   \n",
       "\n",
       "   two_adults_one_working_one_kid_poverty_wage  \\\n",
       "0                                        10.44   \n",
       "1                                        10.44   \n",
       "2                                        13.05   \n",
       "3                                        10.44   \n",
       "4                                        10.44   \n",
       "\n",
       "   two_adults_one_working_two_kids_poverty_wage  \\\n",
       "0                                         12.60   \n",
       "1                                         12.60   \n",
       "2                                         15.75   \n",
       "3                                         12.60   \n",
       "4                                         12.60   \n",
       "\n",
       "   two_adults_one_working_three_kids_poverty_wage  \\\n",
       "0                                           14.75   \n",
       "1                                           14.75   \n",
       "2                                           18.44   \n",
       "3                                           14.75   \n",
       "4                                           14.75   \n",
       "\n",
       "   two_adults_both_working_no_kids_poverty_wage  \\\n",
       "0                                          4.14   \n",
       "1                                          4.14   \n",
       "2                                          5.18   \n",
       "3                                          4.14   \n",
       "4                                          4.14   \n",
       "\n",
       "   two_adults_both_working_one_kid_poverty_wage  \\\n",
       "0                                          5.22   \n",
       "1                                          5.22   \n",
       "2                                          6.53   \n",
       "3                                          5.22   \n",
       "4                                          5.22   \n",
       "\n",
       "   two_adults_both_working_two_kids_poverty_wage  \\\n",
       "0                                           6.30   \n",
       "1                                           6.30   \n",
       "2                                           7.87   \n",
       "3                                           6.30   \n",
       "4                                           6.30   \n",
       "\n",
       "   two_adults_both_working_three_kids_poverty_wage  \n",
       "0                                             7.38  \n",
       "1                                             7.38  \n",
       "2                                             9.22  \n",
       "3                                             7.38  \n",
       "4                                             7.38  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minwage = pd.read_csv('LivingWageStateCapitals.csv.xls')\n",
    "df_minwage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ffb80",
   "metadata": {},
   "source": [
    "Again, we want to select the columns we need which in this case is the state column to merge along and the minimum_wage column which has the minimum wage data we desire (in dollars). Here, we will also rename the \"state_territory\" column to have the same title \"state\" as our original dataframe to streamline the merging process. Then after we complete the inner merge we will verify our final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "335cd874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We still have: 0 NaN entires\n",
      "(30081, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>above_average</th>\n",
       "      <th>Nonfamily Households Median Income (Dollars)</th>\n",
       "      <th>biden_pct</th>\n",
       "      <th>trump_pct</th>\n",
       "      <th>minimum_wage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585407.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>735851.0</td>\n",
       "      <td>Queen Creek</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>85142.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54890.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>49.1</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>961740.0</td>\n",
       "      <td>Sumner</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40625.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>53.2</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>930049.0</td>\n",
       "      <td>Spring Valley</td>\n",
       "      <td>California</td>\n",
       "      <td>91977.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50286.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>34.3</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1826673.0</td>\n",
       "      <td>Maggie Valley</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>28751.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42108.0</td>\n",
       "      <td>48.7</td>\n",
       "      <td>50.1</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>872293.0</td>\n",
       "      <td>Bigfork</td>\n",
       "      <td>Montana</td>\n",
       "      <td>59911.0</td>\n",
       "      <td>2822.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42860.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>56.9</td>\n",
       "      <td>8.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bed  bath  acre_lot     street           city           state  \\\n",
       "0  585407.0  4.0   3.0      0.15   735851.0    Queen Creek         Arizona   \n",
       "1  298000.0  3.0   2.0      1.55   961740.0         Sumner            Iowa   \n",
       "2  739900.0  4.0   2.0      0.25   930049.0  Spring Valley      California   \n",
       "3  315000.0  3.0   2.0      0.42  1826673.0  Maggie Valley  North Carolina   \n",
       "4  855000.0  3.0   2.0      0.22   872293.0        Bigfork         Montana   \n",
       "\n",
       "   zip_code  house_size  above_average  \\\n",
       "0   85142.0      2186.0              1   \n",
       "1   50674.0      1240.0              1   \n",
       "2   91977.0      1340.0              0   \n",
       "3   28751.0      1344.0              0   \n",
       "4   59911.0      2822.0              1   \n",
       "\n",
       "   Nonfamily Households Median Income (Dollars)  biden_pct  trump_pct  \\\n",
       "0                                       54890.0       49.4       49.1   \n",
       "1                                       40625.0       45.0       53.2   \n",
       "2                                       50286.0       63.5       34.3   \n",
       "3                                       42108.0       48.7       50.1   \n",
       "4                                       42860.0       40.6       56.9   \n",
       "\n",
       "   minimum_wage  \n",
       "0         12.00  \n",
       "1          7.25  \n",
       "2         12.00  \n",
       "3          7.25  \n",
       "4          8.65  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select only features we want\n",
    "df_minwage = df_minwage[[\"state_territory\", \"minimum_wage\"]]\n",
    "\n",
    "#rename state_territory column\n",
    "df_minwage = df_minwage.rename(columns={'state_territory': 'state'})\n",
    "\n",
    "#merge dataframe along the column of state\n",
    "df = pd.merge(df, df_minwage, on = [\"state\"])\n",
    "\n",
    "#verify there were no null data values added\n",
    "print (f'We still have: {df.isnull().sum().sum()} NaN entires')\n",
    "\n",
    "#output model summary\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d32f6472-9e47-4856-8c83-c1ee2696cde8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "above_average    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"above_average\"]].agg(\"nunique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca62197",
   "metadata": {},
   "source": [
    "Now we are officially done with merging our dataset and have plenty of new columns to work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96c63b",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "(Just writing some notes for us to use later)\n",
    "- maybe create a fancy visual heatmap type thing showing our prices by zipcode on the us map\n",
    "- Create bar plots, histograms, correlation plots etc using tangible factors from our og dataset (room number, acres etc) should show clear trend\n",
    "- Same thing but for some intangible factors using newly merged data, see if we can come to cool conclusions about those correlations\n",
    "- Writeup analysis about what this shows us about society/housing market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258a0e8-9eca-4f81-a0f0-5654903c5f66",
   "metadata": {},
   "source": [
    "## Prep For Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb210ea-d1b0-44fa-abcc-07b760104d6a",
   "metadata": {},
   "source": [
    "Our code uses categorical, non-numerical columns, which doesnt work with PCA. To allow us to use dimesnion reduction tecniques such as PCA or kernel PCA, we must assign our categorical names into binary/numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce11cf5-6f3d-41e5-b624-6e0d3d1c4d5e",
   "metadata": {},
   "source": [
    "To do this we first must determine which columns we need to change, meaning we need to check which columns are categorical and which are numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b53b6837-52d7-4f42-b8d3-42bbb81d2e86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['price', 'bed', 'bath', 'acre_lot', 'street', 'zip_code', 'house_size', 'above_average', 'Nonfamily Households Median Income (Dollars)', 'biden_pct', 'trump_pct', 'minimum_wage']\n",
      "Categorical columns: ['city', 'state']\n"
     ]
    }
   ],
   "source": [
    "# these two lists hold the columns that are numerical versus categorical\n",
    "numeric = []\n",
    "categoric = []\n",
    "\n",
    "# iterate through the dataframe and sort the column into the numeric list if the type is int or float, otherwise sorting it into the categorical list.\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == np.float64 or df[col].dtype == np.int64:\n",
    "        numeric.append(col)\n",
    "    else:\n",
    "        categoric.append(col)\n",
    "\n",
    "# print\n",
    "print('Numeric columns:', numeric)\n",
    "print('Categorical columns:', categoric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3800f1f-1c7f-419b-9fe3-2e695517dcc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6721cae-38cc-4103-9eb8-6788a1c53cfa",
   "metadata": {},
   "source": [
    "After determining which columns are categorical, and therefore need to be changed to numeric values, we applied hot encoding as a first attempt to make our data more usable for further processes such as pca or neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5cf65b49-b970-45cc-9b24-e572d548eca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>above_average</th>\n",
       "      <th>Nonfamily Households Median Income (Dollars)</th>\n",
       "      <th>biden_pct</th>\n",
       "      <th>...</th>\n",
       "      <th>state_South Dakota</th>\n",
       "      <th>state_Tennessee</th>\n",
       "      <th>state_Texas</th>\n",
       "      <th>state_Utah</th>\n",
       "      <th>state_Vermont</th>\n",
       "      <th>state_Virginia</th>\n",
       "      <th>state_Washington</th>\n",
       "      <th>state_West Virginia</th>\n",
       "      <th>state_Wisconsin</th>\n",
       "      <th>state_Wyoming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585407.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>735851.0</td>\n",
       "      <td>85142.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54890.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>961740.0</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40625.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>930049.0</td>\n",
       "      <td>91977.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50286.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1826673.0</td>\n",
       "      <td>28751.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42108.0</td>\n",
       "      <td>48.7</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>872293.0</td>\n",
       "      <td>59911.0</td>\n",
       "      <td>2822.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42860.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bed  bath  acre_lot     street  zip_code  house_size  \\\n",
       "0  585407.0  4.0   3.0      0.15   735851.0   85142.0      2186.0   \n",
       "1  298000.0  3.0   2.0      1.55   961740.0   50674.0      1240.0   \n",
       "2  739900.0  4.0   2.0      0.25   930049.0   91977.0      1340.0   \n",
       "3  315000.0  3.0   2.0      0.42  1826673.0   28751.0      1344.0   \n",
       "4  855000.0  3.0   2.0      0.22   872293.0   59911.0      2822.0   \n",
       "\n",
       "   above_average  Nonfamily Households Median Income (Dollars)  biden_pct  \\\n",
       "0              1                                       54890.0       49.4   \n",
       "1              1                                       40625.0       45.0   \n",
       "2              0                                       50286.0       63.5   \n",
       "3              0                                       42108.0       48.7   \n",
       "4              1                                       42860.0       40.6   \n",
       "\n",
       "   ...  state_South Dakota  state_Tennessee  state_Texas  state_Utah  \\\n",
       "0  ...               False            False        False       False   \n",
       "1  ...               False            False        False       False   \n",
       "2  ...               False            False        False       False   \n",
       "3  ...               False            False        False       False   \n",
       "4  ...               False            False        False       False   \n",
       "\n",
       "   state_Vermont  state_Virginia  state_Washington  state_West Virginia  \\\n",
       "0          False           False             False                False   \n",
       "1          False           False             False                False   \n",
       "2          False           False             False                False   \n",
       "3          False           False             False                False   \n",
       "4          False           False             False                False   \n",
       "\n",
       "   state_Wisconsin  state_Wyoming  \n",
       "0            False          False  \n",
       "1            False          False  \n",
       "2            False          False  \n",
       "3            False          False  \n",
       "4            False          False  \n",
       "\n",
       "[5 rows x 6120 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a copy of the data so that we dont affect the origional dataframe\n",
    "data = df.copy()\n",
    "\n",
    "# one hot encode categorical features that we discovered in the last cell\n",
    "one_hot_encoded_data = pd.get_dummies(data, columns = ['city', 'state'])\n",
    "\n",
    "# print our data to understand what we are working with\n",
    "one_hot_encoded_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d0e0a8-de15-4b50-a012-e2c03d4ac75f",
   "metadata": {},
   "source": [
    "Above, we can see that the hot encoding has taken the two categorical columns, and seperated each of the different values into their own unique column, and assigned binary(true/false) to their association with the values of other features. Even though this has done what we were interested in it accomplishing - making the categorical columns numerical - there are way too many now, and the data is too complex. Instead, below we will use another method - label encoding.\n",
    "\n",
    "\"Very high-dimensional data that is created from one-hot encoding means that these models are very computationally expensive and unable to be run in our project currently.\n",
    "\n",
    "So, we will label encode our categorical variables for Support Vector Machines and Logistic Regression and prep the data in the same way as for the neural network.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56d39a81-3ab4-411f-80c5-67443e173abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # split into test/train data\n",
    "# X = one_hot_encoded_data.drop(['above_average'], axis = 1)\n",
    "# y = one_hot_encoded_data['above_average']\n",
    "# train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# # Scale the numeric features before converting to arrays\n",
    "# numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "# scaler = StandardScaler()\n",
    "# train_X[numeric_features] = scaler.fit_transform(train_X[numeric_features])\n",
    "# test_X[numeric_features] = scaler.transform(test_X[numeric_features])\n",
    "\n",
    "# # Convert to arrays for the model\n",
    "# X_train = np.array(train_X, dtype=np.float32)\n",
    "# X_test = np.array(test_X, dtype=np.float32)\n",
    "# y_train = tf.keras.utils.to_categorical(train_y).astype(np.int64)\n",
    "# y_test = tf.keras.utils.to_categorical(test_y).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e739cb-37ab-4b07-991a-4bf576cbb63d",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70025ba5-df2d-40c8-9aea-80830a0a607e",
   "metadata": {},
   "source": [
    "Label encoding further simplifies the number of columns for kernel pca, as the hot encoding creates too many columns to be used. In label encoding, each unique category value is assigned an integer - making our data numerical and able to be used in PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc40a019-24c9-4fd0-98eb-476f3488af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# create the label encoded data\n",
    "label_encoded_data = df.copy()\n",
    "label_encoded_data['state'] = label_encoder.fit_transform(label_encoded_data['state'])\n",
    "label_encoded_data['city'] = label_encoder.fit_transform(label_encoded_data['city'])\n",
    "\n",
    "# create X and y data\n",
    "X_le = label_encoded_data.drop(['above_average'], axis=1)\n",
    "y_le = label_encoded_data['above_average']\n",
    "\n",
    "# split into train and test sets\n",
    "#set the random state to be the same for all train test splits so we know our results are from the same data.\n",
    "train_Xle, test_Xle, train_yle, test_yle = train_test_split(X_le, y_le, test_size=0.33, random_state=42)\n",
    "\n",
    "# scale the numeric features\n",
    "scaler = StandardScaler()\n",
    "train_Xle = scaler.fit_transform(train_Xle)\n",
    "test_Xle = scaler.transform(test_Xle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e22b53c6-8082-4ed4-b26a-d0ee892b7ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bed</th>\n",
       "      <th>bath</th>\n",
       "      <th>acre_lot</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>house_size</th>\n",
       "      <th>above_average</th>\n",
       "      <th>Nonfamily Households Median Income (Dollars)</th>\n",
       "      <th>biden_pct</th>\n",
       "      <th>trump_pct</th>\n",
       "      <th>minimum_wage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585407.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>735851.0</td>\n",
       "      <td>4451</td>\n",
       "      <td>2</td>\n",
       "      <td>85142.0</td>\n",
       "      <td>2186.0</td>\n",
       "      <td>1</td>\n",
       "      <td>54890.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>49.1</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>961740.0</td>\n",
       "      <td>5260</td>\n",
       "      <td>15</td>\n",
       "      <td>50674.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40625.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>53.2</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>739900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>930049.0</td>\n",
       "      <td>5145</td>\n",
       "      <td>4</td>\n",
       "      <td>91977.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50286.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>34.3</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1826673.0</td>\n",
       "      <td>3170</td>\n",
       "      <td>33</td>\n",
       "      <td>28751.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42108.0</td>\n",
       "      <td>48.7</td>\n",
       "      <td>50.1</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>855000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>872293.0</td>\n",
       "      <td>439</td>\n",
       "      <td>26</td>\n",
       "      <td>59911.0</td>\n",
       "      <td>2822.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42860.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>56.9</td>\n",
       "      <td>8.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30076</th>\n",
       "      <td>240000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1560549.0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>21520.0</td>\n",
       "      <td>1512.0</td>\n",
       "      <td>0</td>\n",
       "      <td>34758.0</td>\n",
       "      <td>65.8</td>\n",
       "      <td>32.4</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30077</th>\n",
       "      <td>259999.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1362707.0</td>\n",
       "      <td>2864</td>\n",
       "      <td>13</td>\n",
       "      <td>60156.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60107.0</td>\n",
       "      <td>57.6</td>\n",
       "      <td>40.5</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30078</th>\n",
       "      <td>1650000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.97</td>\n",
       "      <td>589967.0</td>\n",
       "      <td>2110</td>\n",
       "      <td>4</td>\n",
       "      <td>95949.0</td>\n",
       "      <td>4934.0</td>\n",
       "      <td>1</td>\n",
       "      <td>39872.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>34.3</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30079</th>\n",
       "      <td>445900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1782368.0</td>\n",
       "      <td>4916</td>\n",
       "      <td>9</td>\n",
       "      <td>33777.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52365.0</td>\n",
       "      <td>47.9</td>\n",
       "      <td>51.2</td>\n",
       "      <td>8.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30080</th>\n",
       "      <td>285000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>762889.0</td>\n",
       "      <td>5151</td>\n",
       "      <td>9</td>\n",
       "      <td>34609.0</td>\n",
       "      <td>1809.0</td>\n",
       "      <td>0</td>\n",
       "      <td>36195.0</td>\n",
       "      <td>47.9</td>\n",
       "      <td>51.2</td>\n",
       "      <td>8.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30081 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           price  bed  bath  acre_lot     street  city  state  zip_code  \\\n",
       "0       585407.0  4.0   3.0      0.15   735851.0  4451      2   85142.0   \n",
       "1       298000.0  3.0   2.0      1.55   961740.0  5260     15   50674.0   \n",
       "2       739900.0  4.0   2.0      0.25   930049.0  5145      4   91977.0   \n",
       "3       315000.0  3.0   2.0      0.42  1826673.0  3170     33   28751.0   \n",
       "4       855000.0  3.0   2.0      0.22   872293.0   439     26   59911.0   \n",
       "...          ...  ...   ...       ...        ...   ...    ...       ...   \n",
       "30076   240000.0  3.0   2.0      3.00  1560549.0     7     20   21520.0   \n",
       "30077   259999.0  2.0   2.0      0.14  1362707.0  2864     13   60156.0   \n",
       "30078  1650000.0  4.0   3.0     21.97   589967.0  2110      4   95949.0   \n",
       "30079   445900.0  4.0   2.0      0.14  1782368.0  4916      9   33777.0   \n",
       "30080   285000.0  4.0   2.0      0.23   762889.0  5151      9   34609.0   \n",
       "\n",
       "       house_size  above_average  \\\n",
       "0          2186.0              1   \n",
       "1          1240.0              1   \n",
       "2          1340.0              0   \n",
       "3          1344.0              0   \n",
       "4          2822.0              1   \n",
       "...           ...            ...   \n",
       "30076      1512.0              0   \n",
       "30077      2000.0              0   \n",
       "30078      4934.0              1   \n",
       "30079      1932.0              1   \n",
       "30080      1809.0              0   \n",
       "\n",
       "       Nonfamily Households Median Income (Dollars)  biden_pct  trump_pct  \\\n",
       "0                                           54890.0       49.4       49.1   \n",
       "1                                           40625.0       45.0       53.2   \n",
       "2                                           50286.0       63.5       34.3   \n",
       "3                                           42108.0       48.7       50.1   \n",
       "4                                           42860.0       40.6       56.9   \n",
       "...                                             ...        ...        ...   \n",
       "30076                                       34758.0       65.8       32.4   \n",
       "30077                                       60107.0       57.6       40.5   \n",
       "30078                                       39872.0       63.5       34.3   \n",
       "30079                                       52365.0       47.9       51.2   \n",
       "30080                                       36195.0       47.9       51.2   \n",
       "\n",
       "       minimum_wage  \n",
       "0             12.00  \n",
       "1              7.25  \n",
       "2             12.00  \n",
       "3              7.25  \n",
       "4              8.65  \n",
       "...             ...  \n",
       "30076         11.00  \n",
       "30077         10.00  \n",
       "30078         12.00  \n",
       "30079          8.56  \n",
       "30080          8.56  \n",
       "\n",
       "[30081 rows x 14 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise the label encoded data\n",
    "label_encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b82e9-a982-4800-b178-63d39e508323",
   "metadata": {},
   "source": [
    "As shown above, the label encoded data is much more condensed than the hot encoded data, as each element in the categorical columns have been given a unique value. This data is now ready to be used by dimension reduction models such as PCA or for further machine learning models and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8af96-7191-4f2d-8bba-397ee028d807",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e290e5-658d-4a3d-9d1c-bdf818eb4203",
   "metadata": {},
   "source": [
    "Due to the fact that we have so much data, and so many features, we thought it might be the right decision to do a dimesion reduction on our data so that we could ensure that our models have the highest quality data and can run with the highest accuracy. To start out, we used regular PCA to see how many components were needed to get our cumulative explained variance ratio to be larger than 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcb379-28d5-439d-bba7-784da2507755",
   "metadata": {},
   "source": [
    "#### Normal PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98a16ea9-116f-4e1c-9297-b1c293c4ef39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2417125  0.1835296  0.0933645  0.08111681 0.07776586 0.07473534\n",
      " 0.0728888 ]\n",
      "[0.2417125  0.4252421  0.5186066  0.59972341 0.67748927 0.75222461\n",
      " 0.82511342]\n"
     ]
    }
   ],
   "source": [
    "# call PCA command\n",
    "pca = PCA(n_components = 7) # using 4 components\n",
    "X_pca_sklearn = pca.fit_transform(train_Xle) # fit transform the training data\n",
    "\n",
    "# print explained variance ratio and the cumsum explained variance ratio\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6785a8a5-f02a-4a37-bf3f-1bc6429e905c",
   "metadata": {},
   "source": [
    "After doing the PCA and testing a few n_component values, we found that there had to be 7 n_components in order for there to be a cumulative explained variance ratio larger than 80%. This seemed very large, but we wanted to see if Kernel PCA might be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f77b0-6f09-4d88-989c-6dffb146be0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40962c9-c5f0-4cae-82dc-6c779134be5f",
   "metadata": {},
   "source": [
    "In continuing our journey to imporve our data features and accuracy, we decided to try kernel pca, as the nonlinear dimensionality reduction method seemed like it would be able to capture the complexities of our large dataset.\n",
    "When testing our data, though, we found that it still took at least 6 n_components to get the cumulative sum of explained variance ratio above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "885f3f36-9e56-423a-a7e2-ddc794e08962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28672696 0.13112539 0.1232155  0.12012658 0.11488223 0.09467519\n",
      " 0.07011192 0.05913622]\n",
      "[0.28672696 0.41785236 0.54106786 0.66119444 0.77607667 0.87075186\n",
      " 0.94086378 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# define the KPCA, fit and transform our data based on new features\n",
    "kpca = KernelPCA(n_components = 8, kernel = 'rbf')\n",
    "# kpca_train_Xle = kpca.fit_transform(train_Xle)\n",
    "# kpca_test_Xle = kpca.transform(test_Xle)\n",
    "kpca_transform = kpca.fit_transform(train_Xle)\n",
    "explained_variance = np.var(kpca_transform, axis=0)\n",
    "explained_variance_ratio = explained_variance/ np.sum(explained_variance)\n",
    "print(explained_variance_ratio)\n",
    "print(np.cumsum(explained_variance_ratio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f34fbe-d6cb-479b-a33b-a2ef071c88bd",
   "metadata": {},
   "source": [
    "As we were determined to figure out a way to utilize kernel pca and ensure high quality, cleaned data, we tried to tune the parameters for the RBF kernel in the kernel PCA so that we could find the optimal parameters to reduce the dimensions of our data. To do this, we set a few test gamma values, and set a target variance ratio threshold. Then we loop through the test gammas and output the number of components it takes to get to the target variance threshold, and the cumulative explained variance. We are aiming for the least amounts of n_components to get to the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "806ebed3-a191-4c43-9bb1-a3e91959e211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma: 0.0001, Number of components for 80.0% variance: 7\n",
      "Cumulative explained variance: [0.26493289 0.41976319 0.50997903 0.59559545 0.67872038 0.75340082\n",
      " 0.81531486]\n",
      "Gamma: 0.001, Number of components for 80.0% variance: 7\n",
      "Cumulative explained variance: [0.28724862 0.43379253 0.53233162 0.62604111 0.71711559 0.79756182\n",
      " 0.86282608]\n",
      "Gamma: 0.01, Number of components for 80.0% variance: 7\n",
      "Cumulative explained variance: [0.27263268 0.38778075 0.48546047 0.5790997  0.67020299 0.74867043\n",
      " 0.80790238]\n",
      "Gamma: 0.05, Number of components for 80.0% variance: 22\n",
      "Cumulative explained variance: [0.18993986 0.26957799 0.34636212 0.42136268 0.493155   0.5536854\n",
      " 0.59357747 0.63109514 0.65657642 0.67403168 0.69079382 0.70723569\n",
      " 0.722911   0.73569176 0.74681894 0.75722594 0.766396   0.77411105\n",
      " 0.78140722 0.78849753 0.7954313  0.80198856]\n",
      "Best gamma: 0.0001, Best number of components: 7\n",
      "Explained variance ratio for best model: [0.26493289 0.1548303  0.09021584 0.08561642 0.08312493 0.07468043\n",
      " 0.06191405]\n",
      "Cumulative explained variance for best model: [0.26493289 0.41976319 0.50997903 0.59559545 0.67872038 0.75340082\n",
      " 0.81531486]\n"
     ]
    }
   ],
   "source": [
    "# Tune the gamma parameter for the RBF kernel\n",
    "gamma_values = [0.0001, 0.001, .01, .05] # list of gamma values\n",
    "best_n_components = None # initializing variable\n",
    "best_explained_variance_ratio = None # initializing variable\n",
    "best_gamma = None # initializing variable\n",
    "target_variance_ratio = 0.80  # 80% variance threshold\n",
    "\n",
    "# for loop to iterate through the gamma values and calculate cumulative explained variance\n",
    "for gamma in gamma_values:\n",
    "    kpca2 = KernelPCA(kernel='rbf', gamma=gamma) # create the PCA\n",
    "    kpca_transform = kpca2.fit_transform(train_Xle) # fit and transform the training data\n",
    "    explained_variance = np.var(kpca_transform, axis=0) # calculate explained variance\n",
    "    explained_variance_ratio = explained_variance / np.sum(explained_variance) # calculate the ev ratio\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio) # caluclate cumulative explained variance\n",
    "    \n",
    "    # Find the number of components that meet the target variance ratio\n",
    "    n_components = np.argmax(cumulative_explained_variance >= target_variance_ratio) + 1\n",
    "    \n",
    "    # print the number of components needed for 80% variance using that gamma value\n",
    "    print(f'Gamma: {gamma}, Number of components for {target_variance_ratio*100}% variance: {n_components}')\n",
    "    print(f'Cumulative explained variance: {cumulative_explained_variance[:n_components]}') # output the cumulative explained variance\n",
    "\n",
    "    # fill in best n componenets variable and corresponding best explained variance ratio\n",
    "    if best_n_components is None or n_components < best_n_components:\n",
    "        best_n_components = n_components\n",
    "        best_explained_variance_ratio = explained_variance_ratio\n",
    "        best_gamma = gamma \n",
    "\n",
    "# Fit KPCA with the best parameters\n",
    "kpca2 = KernelPCA(n_components=best_n_components, kernel='rbf', gamma=gamma)\n",
    "kpca_train_Xle = kpca2.fit_transform(train_Xle)\n",
    "kpca_test_Xle = kpca2.transform(test_Xle)\n",
    "\n",
    "# print best gamma value and resulting information\n",
    "print(f'Best gamma: {best_gamma}, Best number of components: {best_n_components}')\n",
    "print(f'Explained variance ratio for best model: {best_explained_variance_ratio[:best_n_components]}')\n",
    "print(f'Cumulative explained variance for best model: {np.cumsum(best_explained_variance_ratio[:best_n_components])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf22e3-f3d6-426b-9cde-308282ee8d51",
   "metadata": {},
   "source": [
    "From the outputs and visualizations, we saw that it was continuously taking large amounts of n_components to get us to the target variance ratio threshold, and the dimension reduction was not condensing our data by much. This meant that it seemed there was no need to preform PCA. This was confirmed as we proceeded in our model making without pca, instead just using the label encoded data, and found that the models were running with high accuracy nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf13c0d-ff92-4f28-bff5-b4dfe2a635a9",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70852e-7701-40b9-8ca6-070d89590e31",
   "metadata": {},
   "source": [
    "## Other Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42523a-1132-4feb-a9b5-961e37544f53",
   "metadata": {},
   "source": [
    "In order to see if we can imporve our accuracy from the neural networks, we decided to run our data through various supervised machine learning models that are used to classify data. These include: SVM, Logistic Regression, K Nearest Neighbors, Decision Tree, Random Forest, and Max Voting Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc1f05-193a-49a7-a077-c695993dde44",
   "metadata": {},
   "source": [
    "#### SVM - Classifier, Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748d484-ffe8-49d5-a7e8-2a330345a794",
   "metadata": {},
   "source": [
    "To find the optimal parameters for the SVM model in order to get us the best accuracy, we set up a grid search, setting various C and gamma values to run thorugh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3dcb262a-0dc7-413e-8048-cbd7f851902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.877 total time=  17.4s\n",
      "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.874 total time=  20.3s\n",
      "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.875 total time=  17.4s\n",
      "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.869 total time=  17.3s\n",
      "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.873 total time=  17.3s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.924 total time=   6.1s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.924 total time=   6.1s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.924 total time=   6.2s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.918 total time=   6.0s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.929 total time=   6.1s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.891 total time=   7.5s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.890 total time=   7.5s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.892 total time=   7.4s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.886 total time=   7.6s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.898 total time=   7.5s\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.832 total time=  10.4s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.834 total time=  10.2s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.832 total time=  10.0s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.824 total time=  10.8s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.842 total time=  10.1s\n",
      "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.895 total time=  27.8s\n",
      "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.894 total time=  24.2s\n",
      "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.893 total time=  24.0s\n",
      "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.888 total time=  23.5s\n",
      "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.891 total time=  23.2s\n",
      "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.956 total time=   5.0s\n",
      "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.948 total time=   5.1s\n",
      "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.955 total time=   5.0s\n",
      "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.950 total time=   5.0s\n",
      "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.956 total time=   5.0s\n",
      "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.919 total time=   5.4s\n",
      "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.920 total time=   5.4s\n",
      "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.918 total time=   5.6s\n",
      "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.916 total time=   5.4s\n",
      "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.926 total time=   5.4s\n",
      "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.876 total time=   7.6s\n",
      "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.885 total time=   7.5s\n",
      "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.880 total time=   7.5s\n",
      "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.881 total time=   7.6s\n",
      "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.890 total time=   7.6s\n",
      "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.892 total time=  22.8s\n",
      "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.888 total time=  22.7s\n",
      "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.892 total time=  22.7s\n",
      "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.890 total time=  22.3s\n",
      "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.889 total time=  22.8s\n",
      "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.963 total time=   5.6s\n",
      "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.963 total time=   5.2s\n",
      "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.966 total time=   5.3s\n",
      "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.961 total time=   5.4s\n",
      "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.962 total time=   5.3s\n",
      "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.933 total time=   5.7s\n",
      "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.928 total time=   5.6s\n",
      "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.929 total time=   5.8s\n",
      "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.929 total time=   5.7s\n",
      "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.937 total time=   5.8s\n",
      "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.904 total time=   6.2s\n",
      "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.906 total time=   6.0s\n",
      "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.904 total time=   6.1s\n",
      "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.900 total time=   5.9s\n",
      "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.911 total time=   6.4s\n",
      "[CV 1/5] END ........C=500, gamma=1, kernel=rbf;, score=0.894 total time=  22.4s\n",
      "[CV 2/5] END ........C=500, gamma=1, kernel=rbf;, score=0.887 total time=  22.7s\n",
      "[CV 3/5] END ........C=500, gamma=1, kernel=rbf;, score=0.894 total time=  22.5s\n",
      "[CV 4/5] END ........C=500, gamma=1, kernel=rbf;, score=0.891 total time=  22.1s\n",
      "[CV 5/5] END ........C=500, gamma=1, kernel=rbf;, score=0.889 total time=  22.4s\n",
      "[CV 1/5] END ......C=500, gamma=0.1, kernel=rbf;, score=0.968 total time=   7.1s\n",
      "[CV 2/5] END ......C=500, gamma=0.1, kernel=rbf;, score=0.959 total time=   6.7s\n",
      "[CV 3/5] END ......C=500, gamma=0.1, kernel=rbf;, score=0.965 total time=   7.2s\n",
      "[CV 4/5] END ......C=500, gamma=0.1, kernel=rbf;, score=0.961 total time=   7.1s\n",
      "[CV 5/5] END ......C=500, gamma=0.1, kernel=rbf;, score=0.962 total time=   6.9s\n",
      "[CV 1/5] END .....C=500, gamma=0.01, kernel=rbf;, score=0.943 total time=   7.2s\n",
      "[CV 2/5] END .....C=500, gamma=0.01, kernel=rbf;, score=0.933 total time=   7.2s\n",
      "[CV 3/5] END .....C=500, gamma=0.01, kernel=rbf;, score=0.938 total time=   7.3s\n",
      "[CV 4/5] END .....C=500, gamma=0.01, kernel=rbf;, score=0.936 total time=   7.2s\n",
      "[CV 5/5] END .....C=500, gamma=0.01, kernel=rbf;, score=0.942 total time=   7.2s\n",
      "[CV 1/5] END ....C=500, gamma=0.001, kernel=rbf;, score=0.916 total time=   6.6s\n",
      "[CV 2/5] END ....C=500, gamma=0.001, kernel=rbf;, score=0.920 total time=   6.2s\n",
      "[CV 3/5] END ....C=500, gamma=0.001, kernel=rbf;, score=0.916 total time=   6.5s\n",
      "[CV 4/5] END ....C=500, gamma=0.001, kernel=rbf;, score=0.912 total time=   6.3s\n",
      "[CV 5/5] END ....C=500, gamma=0.001, kernel=rbf;, score=0.920 total time=   6.6s\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# defining parameter range\n",
    "param_grid = {'C': [1, 10, 100, 500],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001],\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "# setting the grid\n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "\n",
    "# fitting the model for grid search\n",
    "grid.fit(train_Xle, train_yle)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71892133-54dc-4864-8a5b-04580d2fcf12",
   "metadata": {},
   "source": [
    "We find that the above parameters ('C': 100, 'gamma': 0.1) are the most optimal for accuracy, so we input them to our SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d1e31823-1073-41a3-84d1-3105d3dd6b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score is 0.9982137540934802.\n",
      "The testing score is 0.9682683590208522.\n"
     ]
    }
   ],
   "source": [
    "# create SVM model and train\n",
    "svc = SVC(kernel='rbf', C=1000, gamma=0.1)\n",
    "svc.fit(train_Xle, train_yle)\n",
    "\n",
    "# score model on training and test data\n",
    "svm_train_score = svc.score(train_Xle, train_yle)\n",
    "svm_test_score = svc.score(test_Xle, test_yle)\n",
    "svm_preds = svc.predict(test_Xle)\n",
    "\n",
    "# print scores\n",
    "print(f'The training score is {svm_train_score}.')\n",
    "print(f'The testing score is {svm_test_score}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75643efe-6a99-473f-82ac-99bbee83fe77",
   "metadata": {},
   "source": [
    "The training score resulting is extremely high, and although the testing score isnt the exact same, it is still evaluating very high, meaning we arent worried about overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761954a-de64-4a21-b632-e7857c497f15",
   "metadata": {},
   "source": [
    "#### Logistic Regression - Classifier, Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e84328-30f8-4b10-8bd7-dd94b824f2ec",
   "metadata": {},
   "source": [
    "To find the optimal parameters for the Logistic Regression model, we created a parameter grid to test various C values, and ran the grid search in order to find the best parameters for the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "226299e0-6838-4e64-b037-45f58d4ba940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Best cross-validation accuracy: 0.8875661021623362\n",
      "Test set accuracy: 0.8815352070111816\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "log_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 500],\n",
    "    'penalty': ['l2'],  # 'l2' is the only penalty supported by 'newton-cg'\n",
    "    'solver': ['newton-cg']}  # Only solvers compatible with 'l2' penalty\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log = LogisticRegression()\n",
    "\n",
    "# Set up the Grid Search\n",
    "log_grid_search = GridSearchCV(estimator=log, param_grid=log_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the Grid Search to the training data\n",
    "log_grid_search.fit(train_Xle, train_yle)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "log_best_params = log_grid_search.best_params_\n",
    "log_best_score = log_grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters found: {log_best_params}\")\n",
    "print(f\"Best cross-validation accuracy: {log_best_score}\")\n",
    "\n",
    "# Evaluate the model with the best parameters on the test data\n",
    "best_log = log_grid_search.best_estimator_\n",
    "log_y_pred = best_log.predict(test_Xle)\n",
    "log_test_accuracy = accuracy_score(test_yle, log_y_pred)\n",
    "\n",
    "print(f\"Test set accuracy: {log_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05782bf4-069e-4432-93bf-de364f2100b4",
   "metadata": {},
   "source": [
    "The logistic regression model parameters that according to the grid search were most optimal were C=100. This provided us with a lower accuracy(87%) than the SVM model, although the accuracy between train and test was closer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3c996-db3d-4d81-b8cb-055496cf6334",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors - Classifier, Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea896b70-03ee-4ba7-a88a-e9de2e188ddb",
   "metadata": {},
   "source": [
    "To find the optimal parameters for the KNN model, we created a parameter grid to test a few number of neighbor values, and ran the grid search in order to find the best parameter for the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0dcf063a-2499-4a8f-99f4-a2b0557c4cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_neighbors': 20}\n",
      "Best cross-validation accuracy: 0.819043307665838\n",
      "Test set accuracy: 0.8242167825123401\n"
     ]
    }
   ],
   "source": [
    "# accuracy_neigh = {} # create dictionary to hold the accuracy for each nearest neighbors\n",
    "\n",
    "# # for loop to cycle through possible number of nearest neighbors\n",
    "# for i in range (8,36,2):\n",
    "    \n",
    "#     # train model and fit data based on current iterations number of nearest neighbors\n",
    "#     neigh = KNeighborsClassifier(n_neighbors=i).fit(train_Xle, train_yle)\n",
    "\n",
    "#     # calculate test accuracy and print \n",
    "#     y_pred_neigh = neigh.predict(test_Xle)\n",
    "#     acc_neigh = neigh.score(test_Xle, test_yle)\n",
    "#     print(f'Test accuracy for {i} nearest neighbors is {acc_neigh:.2f}')\n",
    "    \n",
    "#     # append accuracy to dictionary with index (number of nearest neighbors)\n",
    "#     accuracy_neigh[i] = acc_neigh\n",
    "\n",
    "# max_accuracy_neigh = max(accuracy_neigh.values()) # find the highest accuracy\n",
    "# accuracy_index_neighbors = max(accuracy_neigh, key=accuracy_neigh.get) # find the highest accuracy's index (number of nearest neighbor)\n",
    "# print(f\"\\nHighest accuracy: {max_accuracy_neigh:.2f}, n_neighbors: {accuracy_index_neighbors}\") # print\n",
    "\n",
    "# # train and fit model based on the highest accuracy\n",
    "# neighbors = KNeighborsClassifier(n_neighbors=accuracy_index_neighbors).fit(train_Xle, train_yle)\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': range(8, 36, 2)}\n",
    "\n",
    "# Initialize the KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Set up the Grid Search\n",
    "knn_grid_search = GridSearchCV(estimator=knn, param_grid=knn_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the Grid Search to the training data\n",
    "knn_grid_search.fit(train_Xle, train_yle)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "knn_best_params = knn_grid_search.best_params_\n",
    "knn_best_score = knn_grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters found: {knn_best_params}\")\n",
    "print(f\"Best cross-validation accuracy: {knn_best_score}\")\n",
    "\n",
    "# Evaluate the model with the best parameters on the test data\n",
    "best_knn = knn_grid_search.best_estimator_\n",
    "knn_y_pred = best_knn.predict(test_Xle)\n",
    "knn_test_accuracy = accuracy_score(test_yle, knn_y_pred)\n",
    "\n",
    "print(f\"Test set accuracy: {knn_test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fd603-de81-42a5-9150-0cfafc451ca6",
   "metadata": {},
   "source": [
    "The KNN model parameters that according to the grid search were most optimal were 'n_neighbors': 12. This provided us with a lower accuracy(80%) than the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff148c-d9b3-4363-94ff-7167c1745310",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0942f268-c111-4e29-a674-265859371360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accuracy_neigh2 = {} # create dictionary to hold the accuracy for each nearest neighbors\n",
    "\n",
    "# # for loop to cycle through possible number of nearest neighbors\n",
    "# for j in range (8,36,2):\n",
    "    \n",
    "#     # train model and fit data based on current iterations number of nearest neighbors\n",
    "#     neigh2 = KNeighborsClassifier(n_neighbors=j).fit(kpca_train_Xle, train_yle)\n",
    "\n",
    "#     # calculate test accuracy and print \n",
    "#     y_pred_neigh = neigh.predict(kpca_test_Xle)\n",
    "#     acc_neigh = neigh.score(kpca_test_Xle, test_yle)\n",
    "#     print(f'Test accuracy for {i} nearest neighbors is {acc_neigh:.2f}')\n",
    "    \n",
    "#     # append accuracy to dictionary with index (number of nearest neighbors)\n",
    "#     accuracy_neigh[i] = acc_neigh\n",
    "\n",
    "# max_accuracy_neigh = max(accuracy_neigh.values()) # find the highest accuracy\n",
    "# accuracy_index_neighbors = max(accuracy_neigh, key=accuracy_neigh.get) # find the highest accuracy's index (number of nearest neighbor)\n",
    "# print(f\"\\nHighest accuracy: {max_accuracy_neigh:.2f}, n_neighbors: {accuracy_index_neighbors}\") # print\n",
    "\n",
    "# # train and fit model based on the highest accuracy\n",
    "# neighbors = KNeighborsClassifier(n_neighbors=accuracy_index_neighbors).fit(kpca_test_Xle, train_yle)\n",
    "\n",
    "# # # plot the decision region\n",
    "# # plot_regions(neighbors, X_test_, y_test, 'K-Nearest-Neighbors Model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d421080-d9ab-4299-93fd-0d20470ee3c2",
   "metadata": {},
   "source": [
    "#### Decision Tree - Clasisfication, Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ee039-964d-4ab2-a57b-8329c3a98288",
   "metadata": {},
   "source": [
    "To find the optimal parameters for the Decision Tree model, we created a parameter grid to test a few max depths, and ran the grid search in order to find the best parameter for the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eef0aff8-a8b7-433b-bcab-9f0d8e1138ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20}\n",
      "Best train accuracy: 0.9920612646530333\n",
      "Test accuracy for the best model: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "tree_param_grid = {\n",
    "    'max_depth': range(2, 21, 2),}\n",
    "\n",
    "# Initialize the grid search\n",
    "tree_grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=tree_param_grid, cv=5)\n",
    "\n",
    "# Perform the grid search\n",
    "tree_grid_search.fit(train_Xle, train_yle)\n",
    "\n",
    "# Get the best parameters\n",
    "tree_best_params = tree_grid_search.best_params_\n",
    "tree_best_score = tree_grid_search.best_score_\n",
    "print(\"Best Parameters:\", tree_best_params)\n",
    "print(\"Best train accuracy:\", tree_best_score)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_tree = DecisionTreeClassifier(**tree_best_params)\n",
    "best_tree.fit(train_Xle, train_yle)\n",
    "\n",
    "# Calculate test accuracy\n",
    "acc_tree = best_tree.score(test_Xle, test_yle)\n",
    "print(f'Test accuracy for the best model: {acc_tree:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984a58f-1fef-47a2-a706-b78d6cc9036c",
   "metadata": {},
   "source": [
    "The Decision Tree model parameters that according to the grid search were most optimal were 'max_depth': 16. This provided us with a very high test accuracy(99%), much higher than any of the previous models. Plus the train and test scores were very similar, again meaning that its unlikely that there is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2200663-649d-4e5d-b4d2-837dd5bd7106",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random Forest - Classification, Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa8697-bb59-45d5-883c-3d222f71780b",
   "metadata": {},
   "source": [
    "To find the optimal parameters for the Random Forest model, we created a parameter grid to test a few number of estimators and max depths, and ran the grid search in order to find the best parameter for the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "789a5faa-9588-4af9-9871-43b50313c0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 107, 'max_depth': 14}\n",
      "Best train accuracy: 0.9876948069336094\n",
      "Test accuracy for the best model: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': range(100, 111),  # Adjust the range as per your preference\n",
    "    'max_depth': range(2, 21, 2)}\n",
    "\n",
    "# Initialize the random search\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=rf_param_grid, n_iter=10, cv=5)\n",
    "\n",
    "# Perform the random search\n",
    "random_search.fit(train_Xle, train_yle)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_rf = random_search.best_params_\n",
    "print(\"Best Parameters:\", best_params_rf)\n",
    "rf_best_score = random_search.best_score_\n",
    "print(\"Best train accuracy:\", rf_best_score)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf = RandomForestClassifier(**best_params_rf)\n",
    "best_rf.fit(train_Xle, train_yle)\n",
    "\n",
    "# Calculate test accuracy\n",
    "acc_rf = best_rf.score(test_Xle, test_yle)\n",
    "print(f'Test accuracy for the best model: {acc_rf:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5bd6f4-4863-4440-8a86-06b5ecb35fea",
   "metadata": {},
   "source": [
    "The Random Forest model parameters that according to the grid search were most optimal were . This provided us with a very high test accuracy(99%), much higher than any of the previous models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d67bcd-c789-42e9-aabb-124f56246a20",
   "metadata": {},
   "source": [
    "#### Max Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10dce7-77c1-470d-85ca-c5ff710a9d10",
   "metadata": {},
   "source": [
    "To find the optimal parameters for the Max Voting Classifier model, we collected all of the models we created above and ran them all together in order to compare their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5b15834d-f18b-477c-bf64-71bfc491aed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier 0.8224035458849602\n",
      "LogisticRegression 0.8817366777475572\n",
      "DecisionTreeClassifier 0.9960713206406769\n",
      "RandomForestClassifier 0.9895235217084718\n",
      "VotingClassifier 0.9571874685201974\n"
     ]
    }
   ],
   "source": [
    "#define classifiers you want to use (single models you want to use), and use the models ran before - as they hold the selected hyperparameters \n",
    "kn_clf = knn\n",
    "log_clf = log\n",
    "tree_clf = best_tree\n",
    "rf_clf = best_rf\n",
    "\n",
    "# define max vote classifier - uses estimators which are all the models you want to use\n",
    "voting_clf = VotingClassifier( estimators=[('kn',kn_clf),('lr',log_clf),('tree',tree_clf), ('rf',rf_clf)], voting='hard')\n",
    "\n",
    "# train max vote classifier - fit data\n",
    "voting_clf.fit(train_Xle, train_yle)\n",
    "\n",
    "# look at and print each classifier's accuracy on the test set:\n",
    "for clf in (kn_clf, log_clf, tree_clf, rf_clf, voting_clf):\n",
    "    clf.fit(train_Xle, train_yle) # fit and train \n",
    "    acc_vc = clf.score(test_Xle, test_yle) # calculate test accuracy\n",
    "    print(clf.__class__.__name__, acc_vc) # print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7e7d3-969f-44d0-a283-3ff18b9b50d2",
   "metadata": {},
   "source": [
    "According to the collection of accuracies we have all together here, the Decision Tree and Random Forest Models have the highest testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592816bd-6ccf-48de-bf99-65b12c8a28a6",
   "metadata": {},
   "source": [
    "# Discussion of these origional models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9ba00-adac-4304-a858-0453c0889c43",
   "metadata": {},
   "source": [
    "## Data Reprocessing and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738c118-8e91-4fed-80dd-232308148c2b",
   "metadata": {},
   "source": [
    "## Preparing NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab82366-85a5-4b8c-abf3-5acfe835d1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Office Hours Notes:\n",
    "\n",
    "- feature selection - do the prediciton on most relavant features\n",
    "- lots of models - do hyperparameter selection, if we use simple nn, tune hyperparamters to improve preformance\n",
    "- compare all models - nn and machine learning, must make sure they are all fully trained - okay if the machine learning models are working better than nn\n",
    "- cross validation - estimates test accuracy - sometimes use this to select parameters - but can also use to estimate test accuracy\n",
    "--- do a bunch of diff splitting and see if keeps preforming highest\n",
    "\n",
    "\n",
    "Presentation\n",
    "- whats our goal\n",
    "- data, models, method, what did we do, what did we try"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
